{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material de apoio complementar ao conteúdo oficial\n",
    "\n",
    "## Designing and Implementing a Data Science Solution on Azure\n",
    "\n",
    "Contato: sidneyocirqueira@gmail.com\n",
    "\n",
    "Linkedin: https://www.linkedin.com/in/sidneyoliveiracirqueira/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para criar uma conta gratuita no Microsoft Azure, siga estas etapas:\n",
    "\n",
    "    •         Acesse https://azure.microsoft.com/en-us/free/\n",
    "\n",
    "    •         Clique em Start free\n",
    "\n",
    "    •         Crie sua conta e/ou entre com uma conta registrada\n",
    "    \n",
    "        o    Obs. É necessária uma conta da Microsoft (anteriormente conhecida como Windows Live ID) que não tenha sido usada com uma oferta de                      avaliação anterior do Windows Azure. Se você não tiver uma conta da Microsoft qualificada, acesse: https://signup.live.com para se                  inscrever em uma.\n",
    "    \n",
    "    •         Insira o código recebido (se necessário para validar a criação da sua conta).\n",
    "\n",
    "    •        Informe seus dados de identificação e informações de cartão de crédito\n",
    "\n",
    "        o    Obs. Não haverá cobrança no seu cartão durante os 12 meses de conta gratuito. Após esse periodo você precisará ativar a conta                            paga por utilização para que receba cobranças no cartão de crédito.\n",
    "\n",
    "    •         Leia e concorde com os politicas de subscrição, detalhes e privacidade.\n",
    "\n",
    "        o    Obs. Depois de enviar o formulário de solicitação, você receberá um e-mail de boas vindas ao Azure e terá acesso a documentação. Seu                     acesso ao Azure começará imediatamente.\n",
    "\n",
    "    •         Clique em Sign up.\n",
    "\n",
    "    •         Clique em Go to portal  e já terá acesso a sua subscrição de gratuita. \n",
    "\n",
    "    •         Link para acesso direto ao Portal do Azure: http://portal.azure.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books: \n",
    "\n",
    "Automated Machine Learning: https://www.amazon.com/Practical-Automated-Machine-Learning-Azure/dp/49205559X\n",
    "\n",
    "Introducing Machine Learning: https://www.microsoftpressstore.com/store/introducing-machine-learning-9780135565667\n",
    "\n",
    "Data Science do Zero: https://www.amazon.com.br/Data-Science-Zero-Primeiras-Regras-ebook/dp/B07Y3ZQQGZ/ref=asc_df_B07Y3ZQQGZ/?tag=googleshopp00-20&linkCode=df0&hvadid=379751728744&hvpos=&hvnetw=g&hvrand=4449222644155540811&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1001541&hvtargid=pla-849887322915&psc=1\n",
    "\n",
    "Aprendizado de Maquina para leigos: https://www.amazon.com.br/Aprendizado-M%C3%A1quina-Para-Leigos-Mueller/dp/8550802344/ref=asc_df_8550802344/?tag=googleshopp00-20&linkCode=df0&hvadid=379787347388&hvpos=&hvnetw=g&hvrand=17921363900870948199&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1001541&hvtargid=pla-836793368530&psc=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certificação\n",
    "\n",
    "Exam DP-100: https://docs.microsoft.com/en-us/learn/certifications/exams/dp-100\n",
    "\n",
    "Medium: https://medium.com/data-hackers/material-de-estudo-para-o-exame-de-certifica%C3%A7%C3%A3o-dp-100-designing-and-implementing-a-data-science-8b9049fb9979"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Microsoft \n",
    "\n",
    "### GitHub DP100\n",
    "https://aka.ms/dp100labs\n",
    "\n",
    "### Azure Fundamentals\n",
    "https://docs.microsoft.com/en-us/learn/paths/azure-fundamentals/\n",
    "\n",
    "### Data Science Fundamentals\n",
    "https://docs.microsoft.com/en-us/learn/paths/foundations-data-science/\n",
    "\n",
    "### Machine Learning crash course\n",
    "https://docs.microsoft.com/en-us/learn/paths/ml-crash-course/\n",
    "github: https://github.com/MicrosoftDocs/ms-learn-ml-crash-course-python\n",
    "\n",
    "### Create no-code predictive models with Azure Machine Learning\n",
    "https://docs.microsoft.com/en-us/learn/paths/create-no-code-predictive-models-azure-machine-learning/\n",
    "\n",
    "### Build AI solutions with Azure Machine Learning\n",
    "https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/\n",
    "\n",
    "### Introduction to machine learning with Python and Azure Notebooks\n",
    "https://docs.microsoft.com/en-us/learn/paths/intro-to-ml-with-python/\n",
    "\n",
    "### Get started with Machine Learning an Azure Data Science Virtual Machine\n",
    "https://docs.microsoft.com/en-us/learn/paths/get-started-with-azure-dsvm/\n",
    "\n",
    "### Perform Data Science with Azure Databricks\n",
    "https://docs.microsoft.com/en-us/learn/paths/perform-data-science-azure-databricks/\n",
    "\n",
    "### How to use Azure Machine Learning Studio\n",
    "https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml\n",
    "\n",
    "### MLOps\n",
    "https://github.com/solliancenet/tech-immersion-data-ai/blob/master/ai-exp6/README.md\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/mlops-python\n",
    "\n",
    "https://github.com/Microsoft/MLOpsPython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning documentation\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/?view=azure-devops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the Azure Machine Learning Python SDK\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    " ws = Workspace.create(name='aml-workspace',\n",
    "                subscription_id='123456-abc-123...',\n",
    "                resource_group='aml-resources',\n",
    "                create_resource_group=True,\n",
    "                location='eastus',\n",
    "                sku='enterprise'\n",
    "                ) \n",
    "\n",
    " ## Use the Azure Command Line Interface (CLI) with the Azure Machine Learning CLI extension.\n",
    " az ml workspace create -w 'aml-workspace' -g 'aml-resources'\n",
    "\n",
    " ## Create an Azure resource Manager (ARM) template\n",
    " https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-workspace-template\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Azure Machine Learning SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install azureml-sdk\n",
    "\n",
    "pip install azureml-sdk[notebooks,automl,explain]\n",
    "\n",
    "https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Visual Code & Visual Code Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://code.visualstudio.com/docs/setup/setup-overview\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-setup-vscode-extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conecting to a Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## config file\n",
    "{\n",
    " \"subscription_id\": \"1234567-abcde-890-fgh...\",\n",
    " \"resource_group\": \"aml-resources\",\n",
    " \"workspace_name\": \"aml-workspace\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace ##class\n",
    "\n",
    "ws = Workspace.from_config() ##method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.get(name='aml-workspace',\n",
    " subscription_id='1234567-abcde-890-fgh...',\n",
    " resource_group='aml-resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ComputeTarget, Datastore, Dataset\n",
    "\n",
    "print(\"Compute Targets:\")\n",
    "for compute_name in ws.compute_targets:\n",
    "    compute = ws.compute_targets[compute_name]\n",
    "    print(\"\\t\", compute.name, ':', compute.type)\n",
    "    \n",
    "print(\"Datastores:\")\n",
    "for datastore_name in ws.datastores:\n",
    "    datastore = Datastore.get(ws, datastore_name)\n",
    "    print(\"\\t\", datastore.name, ':', datastore.datastore_type)\n",
    "    \n",
    "print(\"Datasets:\")\n",
    "for dataset_name in list(ws.datasets.keys()):\n",
    "    dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "    print(\"\\t\", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the Workspace Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compute_name in ws.compute_targets:\n",
    " compute = ws.compute_targets[compute_name]\n",
    " print(compute.name, \":\", compute.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDK Documentation\n",
    "https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Azure CLI\n",
    "\n",
    "https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/reference-azure-machine-learning-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models with Designer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Code Modules\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/module-reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming a Service Endpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "def allowSelfSignedHttps(allowed):\n",
    " # bypass the server certificate verification on client side\n",
    " if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    " ssl._create_default_https_context = ssl._create_unverified_context\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "data = {\n",
    " \"Inputs\": {\n",
    "            \"input0\":\n",
    "            [\n",
    "                {\n",
    "                'PatientID': \"1882185\",\n",
    "                'Pregnancies': \"9\",\n",
    "                'PlasmaGlucose': \"104\",\n",
    "                'DiastolicBloodPressure': \"51\",\n",
    "                'TricepsThickness': \"7\",\n",
    "                'SerumInsulin': \"24\",\n",
    "                'BMI': \"27.36983156\",\n",
    "                'DiabetesPedigree': \"1.35047\",\n",
    "                'Age': \"43\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"GlobalParameters\": {\n",
    "        }\n",
    "    }\n",
    "        body = str.encode(json.dumps(data))\n",
    "        url = 'http://10.0.0.1:80/api/v1/service/diabetes_predictor/score'\n",
    "        api_key = 'a1234567890x' # Replace this with the API key for the web service\n",
    "        headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "        req = urllib.request.Request(url, body, headers)\n",
    "        try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "        result = response.read()\n",
    "        print(result)\n",
    "        except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging\n",
    "        the failure\n",
    "        print(error.info())\n",
    "        print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Experiments\n",
    "\n",
    "### The Experiment Run Context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# create an experiment variable\n",
    "experiment = Experiment(workspace = ws, name = \"my-experiment\")\n",
    "\n",
    "# start the experiment\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# experiment code goes here\n",
    "\n",
    "# end the experiment\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Azure ML experiment runs and metrics\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-track-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "import pandas as pd\n",
    "\n",
    "# Create an Azure ML experiment in your workspace\n",
    "experiment = Experiment(workspace = ws, name = 'my-experiment')\n",
    "\n",
    "# Start logging data from the experiment\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# load the dataset and count the rows\n",
    "data = pd.read_csv('data.csv')\n",
    "row_count = (len(data))\n",
    "\n",
    "# Log the row count\n",
    "run.log('observations', row_count)\n",
    "\n",
    "# Complete the experiment\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging with ML Flow\n",
    "\n",
    "Tip: não está no skill measure mas cai na prova\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Viewing Logged Metrics \n",
    "\n",
    "using the RunDetails widget \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get logged metrics\n",
    "metrics = run.get_metrics()\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# The previous code produces output similar to this:\n",
    "# {\n",
    "#   \"observations\": 15000\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing an Experiment Script\n",
    "\n",
    "the script must import the **azureml.core.Run** class and call its **get_context** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Count the rows and log the result\n",
    "row_count = (len(data))\n",
    "run.log('observations', row_count)\n",
    "\n",
    "# Save a sample of the data\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "data.sample(100).to_csv(\"outputs/sample.csv\", index=False, header=True)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an Experiment Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, RunConfiguration, ScriptRunConfig\n",
    "\n",
    "# create a new RunConfig object\n",
    "experiment_run_config = RunConfiguration()\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                script='experiment.py',\n",
    "                run_config=experiment_run_config)\n",
    " \n",
    "# submit the experiment\n",
    "experiment = Experiment(workspace = ws, name = 'my-experiment')\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Script to Train a Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Prepare the dataset\n",
    "diabetes = pd.read_csv('data.csv')\n",
    "X, y = data[['Feature1','Feature2','Feature3']].values, data['Label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Train a logistic regression model\n",
    "reg = 0.1\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an estimator\n",
    "estimator = Estimator(source_directory='experiment_folder',\n",
    "            entry_script='training_script.py'\n",
    "            compute_target='local',\n",
    "            conda_packages=['scikit-learn']\n",
    "            )\n",
    "\n",
    "# Create and run an experiment\n",
    "experiment = Experiment(workspace = ws, name = 'training_experiment')\n",
    "run = experiment.submit(config=estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework-Specific Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an estimator\n",
    "estimator = SKLearn(source_directory='experiment_folder',\n",
    "            entry_script='training_script.py'\n",
    "            compute_target='local'\n",
    "            )\n",
    "\n",
    "# Create and run an experiment\n",
    "experiment = Experiment(workspace = ws, name = 'training_experiment')\n",
    "run = experiment.submit(config=estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models with Azure Machine Learning Estimator\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-ml-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Script Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Set regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "\n",
    "# Prepare the dataset\n",
    "diabetes = pd.read_csv('data.csv')\n",
    "X, y = data[['Feature1','Feature2','Feature3']].values, data['Label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Script Arguments to an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an estimator\n",
    "estimator = SKLearn(source_directory='experiment_folder',\n",
    "            entry_script='training_script.py',\n",
    "            script_params = {'--reg_rate': 0.1},\n",
    "            compute_target='local'\n",
    " )\n",
    "# Create and run an experiment\n",
    "experiment = Experiment(workspace = ws, name = 'training_experiment')\n",
    "run = experiment.submit(config=estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"run\" is a reference to a completed experiment run\n",
    "\n",
    "# List the files generated by the experiment\n",
    "for file in run.get_file_names():\n",
    " print(file)\n",
    " \n",
    "# Download a named file\n",
    "run.download_file(name='outputs/model.pkl', output_file_path='model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "model = Model.register(workspace=ws,\n",
    "        model_name='classification_model',\n",
    "        model_path='model.pkl', # local path\n",
    "        description='A classification model',\n",
    "        tags={'dept': 'sales'},\n",
    "        model_framework=Model.Framework.SCIKITLEARN,\n",
    "        model_framework_version='0.20.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.register_model( model_name='classification_model',\n",
    "            model_path='outputs/model.pkl', # run outputs path\n",
    "            description='A classification model',\n",
    "            tags={'dept': 'sales'},\n",
    "            model_framework=Model.Framework.SCIKITLEARN,\n",
    "            model_framework_version='0.20.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Datastore\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/concept-data#access-data-in-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering a Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Register a new datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(workspace=ws,\n",
    "                                        datastore_name='blob_data',\n",
    "                                        container_name='data_container',\n",
    "                                        account_name='az_store_acct',\n",
    "                                        account_key='123456abcde789…')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_name in ws.datastores:\n",
    " print(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  using the Datastore.get()\n",
    "\n",
    "blob_store = Datastore.get(ws, datastore_name='blob_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the built-in workspaceblobstore datastore\n",
    "\n",
    "#  get_default_datastore() \n",
    "default_store = ws.get_default_datastore()\n",
    "\n",
    "# To change the default datastore, use the set_default_datastore() method:\n",
    "ws.set_default_datastore('blob_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Directly with Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_ds.upload(src_dir='/files',\n",
    "        target_path='/data/files',\n",
    "        overwrite=True, show_progress=True)\n",
    "blob_ds.download(target_path='downloads',\n",
    "        prefix='/data',\n",
    "        show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Data References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ref = blob_ds.path('data/files').as_download(path_on_compute='training_data')\n",
    "estimator = SKLearn(source_directory='experiment_folder',\n",
    "        entry_script='training_script.py'\n",
    "        compute_target='local',\n",
    "        script_params = {'--data_folder': data_ref})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', type=str, dest='data_folder')\n",
    "args = parser.parse_args()\n",
    "data_files = os.listdir(args.data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Registering Datasets\n",
    "### Creating and Registering Tabular Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the from_delimited_files method of the Dataset.Tabular class \n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "csv_paths = [(blob_ds, 'data/files/current_data.csv'),\n",
    "         (blob_ds, 'data/files/archive/*.csv')]\n",
    "         \n",
    "tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)\n",
    "tab_ds = tab_ds.register(workspace=ws, name='csv_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Registering File Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use the from_files method of the Dataset.File class\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a Registered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get a dataset from the workspace datasets collection\n",
    "ds1 = ws.datasets['csv_table']\n",
    "\n",
    "# Get a dataset by name from the datasets class\n",
    "ds2 = Datasets.get_by_name(ws, 'img_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from Datasets\n",
    "\n",
    "### Working with a Dataset Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tabular datasets\n",
    "\n",
    "df = tab_ds.to_pandas_dataframe()\n",
    "# code to work with dataframe goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with a file dataset, you can use the to_path() method \n",
    "\n",
    "for file_path in file_ds.to_path():\n",
    " print(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing a Dataset to an Experiment Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimator\n",
    "\n",
    "estimator = SKLearn( source_directory='experiment_folder',\n",
    "        entry_script='training_script.py'\n",
    "        compute_target='local',\n",
    "        inputs=[tab_ds.as_named_input('csv_data')],\n",
    "        pip_packages=['azureml-dataprep[pandas]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment script\n",
    "run = Run.get_context()\n",
    "data = run.input_datasets['csv_data'].to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When passing a file dataset, you must specify the access mode.\n",
    "\n",
    "estimator = Estimator( source_directory='experiment_folder',\n",
    "        entry_script='training_script.py'\n",
    "        compute_target='local',\n",
    "        inputs=[img_ds.as_named_input('img_data').as_download(path_on_compute='data')],\n",
    "        pip_packages=['azureml-dataprep[pandas]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a New Version of a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [(blob_ds, 'data/files/images/*.jpg'),\n",
    " (blob_ds, 'data/files/images/*.png')]\n",
    "file_ds = Dataset.File.from_files(path=img_paths)\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a Specific Dataset version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrieve a specific version of a dataset by specifying the version parameter in the get_by_name method of the Dataset class.\n",
    "\n",
    "img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Environment from a Specification File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conda configuration settings in a file named conda.yml\n",
    "\n",
    "name: py_env\n",
    "dependencies:\n",
    " - numpy\n",
    " - pandas\n",
    " - scikit-learn\n",
    " - pip:\n",
    " - azureml-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.from_conda_specification(name='training_environment',\n",
    "                    file_path='./conda.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Environment from an Existing Conda Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.from_existing_conda_environment(name='training_environment',\n",
    "                            conda_environment_name='py_env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Environment by Specifying Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment('training_environment')\n",
    "deps = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy'],\n",
    "                pip_packages=['azureml-defaults'])\n",
    "env.python.conda_dependencies = deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the register method of an Environment object to register an environment\n",
    "\n",
    "env.register(workspace=ws)\n",
    "\n",
    "# You can view the registered environments in your workspace like this:\n",
    "from azureml.core import Environment\n",
    "\n",
    "env_names = Environment.list(workspace=ws)\n",
    "for env_name in env_names:\n",
    " print('Name:',env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and using an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Estimator\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "estimator = Estimator(source_directory='experiment_folder'\n",
    "                entry_script='training_script.py',\n",
    "                compute_target='local',\n",
    "                environment_definition=training_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Curated Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "for env in envs:\n",
    " if env.startswith(\"AzureML\"):\n",
    "    print(\"Name\",env)\n",
    "    print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Compute Targets\n",
    "\n",
    "What are compute targets in Azure Machine Learning\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Managed Compute Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Specify a name for the compute (unique within the workspace)\n",
    "compute_name = 'aml-cluster'\n",
    "\n",
    "# Define compute configuration\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                            min_nodes=0, max_nodes=4,\n",
    "                            vm_priority='dedicated')\n",
    "# Create the compute\n",
    "aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "aml_cluster.wait_for_completion(show_output=True)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full list of AmlCompute configuration options\n",
    "\n",
    "https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute.amlcompute?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaching an Unmanaged Compute Target\n",
    "\n",
    "use the ComputeTarget.attach() method to attach the existing compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Specify a name for the compute (unique within the workspace)\n",
    "compute_name = 'db_cluster'\n",
    "\n",
    "# Define configuration for existing Azure Databricks cluster\n",
    "db_workspace_name = 'db_workspace'\n",
    "db_resource_group = 'db_resource_group'\n",
    "db_access_token = '1234-abc-5678-defg-90...'\n",
    "db_config = DatabricksCompute.attach_configuration(resource_group=db_resource_group,\n",
    "                            workspace_name=db_workspace_name,\n",
    "                            access_token=db_access_token)\n",
    "\n",
    "# Create the compute\n",
    "databricks_compute = ComputeTarget.attach(ws, compute_name, db_config)\n",
    "databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for an Existing Compute Target\n",
    "\n",
    "you can catch the ComputeTargetException exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "compute_name = \"aml-cluster\"\n",
    "\n",
    "# Check if the compute target exists\n",
    "try:\n",
    " aml_cluster = ComputeTarget(workspace=ws, name=compute_name)\n",
    " print('Found existing cluster.')\n",
    "except ComputeTargetException:\n",
    "\n",
    " # If not, create it\n",
    " compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                    max_nodes=4)\n",
    " aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
    " \n",
    "aml_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and use compute targets for model training\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Compute Targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Estimator\n",
    "\n",
    "compute_name = 'aml-cluster'\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "estimator = Estimator(source_directory='experiment_folder',\n",
    "                entry_script='training_script.py',\n",
    "                environment_definition=training_env,\n",
    "                compute_target=compute_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Estimator\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "compute_name = \"aml-cluster\"\n",
    "\n",
    "training_cluster = ComputeTarget(workspace=ws, name=compute_name)\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "\n",
    "estimator = Estimator(source_directory='experiment_folder',\n",
    "                entry_script='training_script.py',\n",
    "                environment_definition=training_env,\n",
    "                compute_target=training_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Steps\n",
    "\n",
    "For a full list of supported step types, see azure.pipeline.steps package documentation\n",
    "\n",
    "https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Steps in a Pipeline\n",
    "\n",
    "The following code defines a PythonScriptStep step that runs a script, and an EstimatorStep step that runs an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                source_directory = 'scripts',\n",
    "                script_name = 'data_prep.py',\n",
    "                compute_target = 'aml-cluster',\n",
    "                runconfig = run_config)\n",
    "\n",
    "# Step to run an estimator\n",
    "step2 = EstimatorStep(name = 'train model',\n",
    " estimator = sk_estimator,\n",
    " compute_target = 'aml-cluster')\n",
    "\n",
    "# After defining the steps, you can assign them to a pipeline, and run it as an experiment:\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Construct the pipeline\n",
    "train_pipeline = Pipeline(workspace = ws, steps = [step1,step2])\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
    "pipeline_run = experiment.submit(train_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a PipelineData object that for the preprocessed data that must be passed between the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Define the PipelineData object\n",
    "data_store = ws.get_default_datastore()\n",
    "prepped = PipelineData('prepped_data', datastore=data_store)\n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                source_directory = 'scripts',\n",
    "                script_name = 'data_prep.py',\n",
    "                compute_target = 'aml-cluster',\n",
    "                runconfig = run_config,\n",
    "                # Specify PipelineData as output\n",
    "                outputs=[prepped],\n",
    "                # Pass as data reference to script\n",
    "                arguments = ['--folder', prepped])\n",
    "\n",
    "# Step to run an estimator\n",
    "step2 = EstimatorStep(name = 'train model',\n",
    "                estimator = sk_estimator,\n",
    "                compute_target = 'aml-cluster',\n",
    "                # Specify PipelineData as input\n",
    "                inputs=[prepped],\n",
    "                # Pass as data reference to estimator script\n",
    "                estimator_entry_script_arguments=['--folder', prepped])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Step Output Reuse\n",
    "\n",
    "You can set the allow_reuse parameter in the step configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                source_directory = 'scripts',\n",
    "                script_name = 'data_prep.py',\n",
    "                compute_target = 'aml-cluster',\n",
    "                runconfig = run_config,\n",
    "                outputs=[prepped],\n",
    "                arguments = ['--folder', prepped],\n",
    "                # Disable step reuse\n",
    "                allow_reuse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forcing All Steps to Run\n",
    "\n",
    "Setting the regenerate_outputs parameter when submitting the pipeline experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent run of the pipeline\n",
    "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
    "run = list(pipeline_experiment.get_runs())[0]\n",
    "\n",
    "# Publish the pipeline from the run\n",
    "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
    "                        description='Model training pipeline',\n",
    "                        version='1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Published Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.post(rest_endpoint,\n",
    "                headers=auth_header,\n",
    "                json={\"ExperimentName\": \"run_training_pipeline\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Parameters for a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)\n",
    "\n",
    "## ...\n",
    "\n",
    "step2 = EstimatorStep(name = 'train model',\n",
    "            estimator = sk_estimator,\n",
    "            compute_target = 'aml-cluster',\n",
    "            inputs=[prepped],\n",
    "            estimator_entry_script_arguments=['--folder', prepped,\n",
    "                                '--reg', reg_param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Pipeline with a Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(rest_endpoint,\n",
    "                headers=auth_header,\n",
    "                json={\"ExperimentName\": \"run_training_pipeline\",\n",
    "                \"ParameterAssignments\": {\"reg_rate\": 0.1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling a Pipeline for Periodic Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "daily = ScheduleRecurrence(frequency='Day', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Daily Training',\n",
    "                description='trains model every day',\n",
    "                pipeline_id=published_pipeline.id,\n",
    "                experiment_name='Training_Pipeline',\n",
    "                recurrence=daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggering a Pipeline Run on Data Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                        description='trains model on data change',\n",
    "                        pipeline_id=published_pipeline_id,\n",
    "                        experiment_name='Training_Pipeline',\n",
    "                        datastore=training_datastore,\n",
    "                        path_on_datastore='data/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and deploy machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Pipelines for CI/CD\n",
    "\n",
    "https://github.com/MicrosoftDocs/pipelines-azureml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Real-Time Inferencing Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Register a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "classification_model = Model.register(workspace=ws,\n",
    "                 model_name='classification_model',\n",
    "                 model_path='model.pkl', # local path\n",
    "                 description='A classification model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.register_model( model_name='classification_model',\n",
    "            model_path='outputs/model.pkl', # run outputs path\n",
    "            description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define an Inference Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    " global model\n",
    "\n",
    " # Get the path to the registered model file and load it\n",
    " model_path = Model.get_model_path('classification_model')\n",
    " model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "\n",
    " # Get the input data as a numpy array\n",
    " data = np.array(json.loads(raw_data)['data'])\n",
    "\n",
    " # Get a prediction from the model\n",
    " predictions = model.predict(data)\n",
    " \n",
    " # Return the predictions as any JSON serializable format\n",
    " return predictions.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Add the dependencies for your model\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = 'service_files/env.yml'\n",
    "with open(env_file,\"w\") as f:\n",
    " f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Script and Environment in an InferenceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                source_directory = 'service_files',\n",
    "                                entry_script=\"score.py\",\n",
    "                                conda_file=\"env.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a Deployment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                        memory_gb = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deploy a machine learning model to Azure Functions\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deploy the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                name = 'classifier-service',\n",
    "                models = [classification_model],\n",
    "                inference_config = classifier_inference_config,\n",
    "                deployment_config = classifier_deploy_config,\n",
    "                deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy models with Azure Machine Learning\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Azure Machine Learning SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    " [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "response = service.run(input_data = json_data)\n",
    "\n",
    "# Get the predictions\n",
    "predictions = json.loads(response)\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    " print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a REST Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the scoring_uri property of the Webservice object in the\n",
    "SDK\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    " [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint,\n",
    " data = json_data,\n",
    " headers = request_headers)\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    " print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can retrieve the keys for a service by using the get_keys method of the WebService object associated with the service:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key, secondary_key = service.get_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For token-based authentication, your client application needs to use service-principal authentication to\n",
    "verify its identity through Azure Active Directory (Azure AD) and call the get_token method of the service\n",
    "to retrieve a time-limited token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an authenticated call to the service's REST endpoint, you must include the key or token in the\n",
    "request header like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    " [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { \"Content-Type\":\"application/json\",\n",
    "            \"Authorization\":\"Bearer \" + key_or_token }\n",
    "\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint,\n",
    "                data = json_data,\n",
    "                headers = request_headers)\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    " print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting a Real-Time Inferencing Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Service State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "# Get the deployed service\n",
    "service = AciWebservice(name='classifier-service', workspace=ws)\n",
    "\n",
    "# Check its state\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Service Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to a Local Container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=8890)\n",
    "service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)\n",
    "\n",
    "# You can then test the locally deployed service using the SDK:\n",
    "print(service.run(input_data = json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.reload()\n",
    "print(service.run(input_data = json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Batch Inferencing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=ws,\n",
    "                        model_name='classification_model',\n",
    "                        model_path='model.pkl', # local path\n",
    "                        description='A classification model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  if you have a reference to the Run used to train the model, you can use its register_model method\n",
    "\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an Scoring Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "def init():\n",
    " # Runs when the pipeline step is initialized\n",
    " global model\n",
    "\n",
    " # load the model\n",
    " model_path = Model.get_model_path('classification_model')\n",
    " model = joblib.load(model_path)\n",
    "\n",
    "def run(mini_batch):\n",
    " # This runs for each batch\n",
    "    resultList = []\n",
    "\n",
    " # process each file in the batch\n",
    " for f in mini_batch:\n",
    "    # Read comma-delimited data into an array\n",
    "    data = np.genfromtxt(f, delimiter=',')\n",
    "    # Reshape into a 2-dimensional array for model input\n",
    "    prediction = model.predict(data.reshape(1, -1))\n",
    "    # Append prediction to results\n",
    "    resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    " return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a Pipeline with a ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Get the batch dataset for input\n",
    "batch_data_set = ws.datasets('batch-data')\n",
    "\n",
    "# Set the output location\n",
    "default_ds = ws.get_default_datastore()\n",
    "output_dir = PipelineData(name='inferences',\n",
    " datastore=default_ds,\n",
    " output_path_on_compute='results')\n",
    "\n",
    "# Get the model\n",
    "model = ws.models['classification_model']\n",
    "\n",
    "# Define the parallel run step step configuration\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "        source_directory='batch_scripts',\n",
    "        entry_script=\"batch_scoring_script.py\",\n",
    "        mini_batch_size=\"5\",\n",
    "        error_threshold=10,\n",
    "        output_action=\"append_row\",\n",
    "        environment=batch_env,\n",
    "        compute_target=aml_cluster,\n",
    "        node_count=4)\n",
    "\n",
    "# Create the parallel run step\n",
    "parallelrun_step = ParallelRunStep(\n",
    "                name='batch-score',\n",
    "                models=[model],\n",
    "                parallel_run_config=parallel_run_config,\n",
    "                inputs=[batch_data_set.as_named_input('batch_data')],\n",
    "                output=output_dir,\n",
    "                arguments=[],\n",
    "                allow_reuse=True\n",
    "                )\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the pipeline and Retrieve the Step Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Run the pipeline as an experiment\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "\n",
    "# Get the outputs from the first (and only) step\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='results')\n",
    "\n",
    "# Find the parallel_run_step.txt file\n",
    "for root, dirs, files in os.walk('results'):\n",
    " for file in files:\n",
    "    if file.endswith('parallel_run_step.txt'):\n",
    "        result_file = os.path.join(root,file)\n",
    "\n",
    "# Load and display the results\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publishing a Batch Inferencing Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(name='Batch_Prediction_Pipeline',\n",
    " description='Batch pipeline',\n",
    " version='1.0')\n",
    "rest_endpoint = published_pipeline.endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.post(rest_endpoint,\n",
    " headers=auth_header,\n",
    " json={\"ExperimentName\": \"Batch_Prediction\"})\n",
    "run_id = response.json()[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also schedule the published pipeline to have it run automatically\n",
    "\n",
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "weekly = ScheduleRecurrence(frequency='Week', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\n",
    "                        description='batch inferencing',\n",
    "                        pipeline_id=published_pipeline.id,\n",
    "                        experiment_name='Batch_Prediction',\n",
    "                        recurrence=weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters for your model with Azure Machine Learning\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import choice, normal\n",
    "\n",
    "param_space = {\n",
    " '--batch_size': choice(16, 32, 64),\n",
    " '--learning_rate': normal(10, 3)\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, choice\n",
    "\n",
    "param_space = {\n",
    "        '--batch_size': choice(16, 32, 64),\n",
    "        '--learning_rate': choice(0.01, 0.1, 1.0)\n",
    "        }\n",
    "param_sampling = GridParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, normal\n",
    "\n",
    "param_space = {\n",
    "    '--batch_size': choice(16, 32, 64),\n",
    "    '--learning_rate': normal(10, 3)\n",
    "    }\n",
    "param_sampling = RandomParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform\n",
    "param_space = {\n",
    "    '--batch_size': choice(16, 32, 64),\n",
    "    '--learning_rate': uniform(0.5, 0.1)\n",
    "    }\n",
    "param_sampling = BayesianParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Termination Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandit Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_amount = 0.2,\n",
    "                        evaluation_interval=1,\n",
    "                        delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Stopping Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,\n",
    " delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncation Selection Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
    "\n",
    "early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,\n",
    " evaluation_interval=1,\n",
    " delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters with Hyperdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Training Script for Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the training dataset\n",
    "data = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels, and split for training/validatiom\n",
    "X = data[['feature1','feature2','feature3','feature4']].values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Train a logistic regression model with the reg hyperparameter\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate and log accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring and Running a Hyperdrive Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "\n",
    "# Assumes ws, sklearn_estimator and param_sampling are already defined\n",
    "hyperdrive = HyperDriveConfig(estimator=sklearn_estimator,\n",
    "                hyperparameter_sampling=param_sampling,\n",
    "                policy=None,\n",
    "                primary_metric_name='Accuracy',\n",
    "                primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                max_total_runs=6,\n",
    "                max_concurrent_runs=4)\n",
    "experiment = Experiment(workspace = ws, name = 'hyperdrive_training')\n",
    "hyperdrive_run = experiment.submit(config=hyperdrive)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring and Reviewing Hyperdrive Runs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child_run in run.get_children():\n",
    " print(child_run.id, child_run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also list all runs in descending order of performance like this:\n",
    "\n",
    "for child_run in hyperdrive_.get_children_sorted_by_primary_metric():\n",
    " print(child_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To retrieve the best performing run, you can use the following code:\n",
    "\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters for your model with Azure Machine Learning\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/pulse/understanding-mlops-azure-databricks-raki-rahman/\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#aml-databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets#portal-reuse\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretability in Azure Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/slundberg/shap\n",
    "\n",
    "https://github.com/marcotcr/lime\n",
    "\n",
    "https://github.com/interpretml/interpret-community/\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Explanations with an Automated Machine Learning Experiment\n",
    "\n",
    "●  Select the **Explain** best model configuration setting in the user interface.\n",
    "\n",
    "● Specify the **model_explanaibility** option in the **AutoMLConfig** object in the SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(name='Automated ML Experiment',\n",
    "                # <other configuration settings here...>\n",
    "                model_explainability=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Automated Machine Learning Model Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "\n",
    "# Get the best model (2nd item in outputs)\n",
    "best_run, fitted_model = automl_run.get_output()\n",
    "\n",
    "# Get the feature explanations\n",
    "client = ExplanationClient.from_run(best_run)\n",
    "explanation = client.download_model_explanation()\n",
    "feature_importances = explanation.get_feature_importance_dict()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36364bitaa256e93d5474d70b5b22bed571a0030",
   "display_name": "Python 3.6.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}